
@techreport{alroumiAbstractLanguageThought2020,
  type = {Preprint},
  title = {An Abstract Language of Thought for Spatial Sequences in Humans},
  author = {Al Roumi, Fosca and Marti, S{\'e}bastien and Wang, Liping and Amalric, Marie and Dehaene, Stanislas},
  year = {2020},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.01.16.908665},
  abstract = {How do humans encode spatial and sequential information in working memory? We tested the hypothesis that participants do not merely store each consecutive location in a distinct memory slot, but instead compress the whole sequence using an abstract language-like code that captures geometrical regularities at multiple nested levels. We exposed participants to sequences of fixed length but variable regularity, while their brain activity was recorded using magneto-encephalography. The entire sequence could be decoded from brain signals. Anticipation signals could also be decoded, and both behavior and anticipatory brain signals were modulated by sequence complexity, defined as the minimal description length provided by the formal language. Furthermore, abstract primitives of rotation and symmetry could be decoded both in isolation and within the sequences. These results suggest that humans encode the transitions between sequence items as rotations and symmetries, and compress longer sequences using nested repetitions of those primitives.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\LC5QASQG\\Al Roumi et al. - 2020 - Mental compression of spatial sequences in human w.pdf}
}

@article{amalricLanguageGeometryFast2017,
  title = {The Language of Geometry: {{Fast}} Comprehension of Geometrical Primitives and Rules in Human Adults and Preschoolers},
  shorttitle = {The Language of Geometry},
  author = {Amalric, Marie and Wang, Liping and Pica, Pierre and Figueira, Santiago and Sigman, Mariano and Dehaene, Stanislas},
  year = {2017},
  month = jan,
  journal = {PLoS computational biology},
  volume = {13},
  number = {1},
  pages = {e1005273},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005273},
  abstract = {During language processing, humans form complex embedded representations from sequential inputs. Here, we ask whether a "geometrical language" with recursive embedding also underlies the human ability to encode sequences of spatial locations. We introduce a novel paradigm in which subjects are exposed to a sequence of spatial locations on an octagon, and are asked to predict future locations. The sequences vary in complexity according to a well-defined language comprising elementary primitives and recursive rules. A detailed analysis of error patterns indicates that primitives of symmetry and rotation are spontaneously detected and used by adults, preschoolers, and adult members of an indigene group in the Amazon, the Munduruku, who have a restricted numerical and geometrical lexicon and limited access to schooling. Furthermore, subjects readily combine these geometrical primitives into hierarchically organized expressions. By evaluating a large set of such combinations, we obtained a first view of the language needed to account for the representation of visuospatial sequences in humans, and conclude that they encode visuospatial sequences by minimizing the complexity of the structured expressions that capture them.},
  langid = {english},
  pmcid = {PMC5305265},
  pmid = {28125595},
  keywords = {Adult,Algorithms,Child; Preschool,Comprehension,Concept Formation,Culture,Humans,Indians; South American,Language,Male,Mathematical Concepts,Mathematics,Models; Educational,Terminology as Topic},
  file = {C\:\\Users\\faust\\Zotero\\storage\\X8S2CJNQ\\Amalric et al. - 2017 - The language of geometry Fast comprehension of ge.pdf}
}

@article{ellisDreamCoderGrowingGeneralizable2020,
  title = {{{DreamCoder}}: {{Growing}} Generalizable, Interpretable Knowledge with Wake-Sleep {{Bayesian}} Program Learning},
  shorttitle = {{{DreamCoder}}},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sable-Meyer}, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and {Solar-Lezama}, Armando and Tenenbaum, Joshua B.},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.08381 [cs]},
  eprint = {2006.08381},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Expert problem-solving is driven by powerful languages for thinking about problems and their solutions. Acquiring expertise means learning these languages -- systems of concepts, alongside the skills to use them. We present DreamCoder, a system that learns to solve problems by writing programs. It builds expertise by creating programming languages for expressing domain concepts, together with neural networks to guide the search for programs within these languages. A ``wake-sleep'' learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems. DreamCoder solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes. It rediscovers the basics of modern functional programming, vector algebra and classical physics, including Newton's and Coulomb's laws. Concepts are built compositionally from those learned earlier, yielding multi-layered symbolic representations that are interpretable and transferrable to new tasks, while still growing scalably and flexibly with experience.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\faust\\Zotero\\storage\\IUJMABXS\\Ellis et al. - 2020 - DreamCoder Growing generalizable, interpretable k.pdf;C\:\\Users\\faust\\Zotero\\storage\\N5JSYHI8\\2006.html}
}

@article{feldmanCatalogBooleanConcepts2003,
  title = {A Catalog of {{Boolean}} Concepts},
  author = {Feldman, Jacob},
  year = {2003},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {47},
  number = {1},
  pages = {75--89},
  issn = {00222496},
  doi = {10.1016/S0022-2496(02)00025-1},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\EBKX22QP\\Feldman - 2003 - A catalog of Boolean concepts.pdf}
}

@article{feldmanMinimizationBooleanComplexity2000,
  title = {Minimization of {{Boolean}} Complexity in Human Concept Learning},
  author = {Feldman, Jacob},
  year = {2000},
  month = oct,
  journal = {Nature},
  volume = {407},
  number = {6804},
  pages = {630--633},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/35036586},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\D79UAWCL\\Feldman - 2000 - Minimization of Boolean complexity in human concep.pdf}
}

@article{fodorFodorGuideMental1985,
  title = {Fodor's {{Guide}} to {{Mental Representation}}: {{The Intelligent Auntie}}'s {{Vade-Mecum}}},
  shorttitle = {Fodor's {{Guide}} to {{Mental Representation}}},
  author = {Fodor, J. A.},
  year = {1985},
  journal = {Mind},
  volume = {94},
  number = {373},
  pages = {76--100},
  publisher = {{[Oxford University Press, Mind Association]}},
  issn = {0026-4423}
}

@book{FormalGrammarsLinguistics,
  title = {Formal {{Grammars}} in {{Linguistics}} and {{Psycholinguistics}}},
  file = {C\:\\Users\\faust\\Zotero\\storage\\VGRBP7Q3\\content.pdf}
}

@article{goodmanRationalAnalysisRuleBased2008,
  title = {A {{Rational Analysis}} of {{Rule-Based Concept Learning}}},
  author = {Goodman, Noah D. and Tenenbaum, Joshua B. and Feldman, Jacob and Griffiths, Thomas L.},
  year = {2008},
  journal = {Cognitive Science},
  volume = {32},
  number = {1},
  pages = {108--154},
  issn = {1551-6709},
  doi = {10.1080/03640210701802071},
  abstract = {This article proposes a new model of human concept learning that provides a rational analysis of learning feature-based concepts. This model is built upon Bayesian inference for a grammatically structured hypothesis space\textemdash a concept language of logical rules. This article compares the model predictions to human generalization judgments in several well-known category learning experiments, and finds good agreement for both average and individual participant generalizations. This article further investigates judgments for a broad set of 7-feature concepts\textemdash a more natural setting in several ways\textemdash and again finds that the model explains human performance.},
  langid = {english},
  keywords = {Bayesian induction,Categorization,Concept learning,Language of thought,Probabilistic grammar,Rules},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1080/03640210701802071},
  file = {C\:\\Users\\faust\\Zotero\\storage\\VIUMY8AN\\Goodman et al. - 2008 - A Rational Analysis of Rule-Based Concept Learning.pdf;C\:\\Users\\faust\\Zotero\\storage\\PYYSY7XF\\03640210701802071.html}
}

@book{heimSemanticsGenerativeGrammar1998,
  title = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {1998},
  series = {Blackwell Textbooks in Linguistics},
  number = {13},
  publisher = {{Blackwell}},
  address = {{Malden, MA}},
  isbn = {978-0-631-19712-6 978-0-631-19713-3},
  langid = {english},
  lccn = {P325.5.G45 H45 1998},
  keywords = {Generative grammar,Semantics},
  file = {C\:\\Users\\faust\\Zotero\\storage\\BQJFSCUG\\Heim and Kratzer - 1998 - Semantics in generative grammar.pdf}
}

@article{lakeBuildingMachinesThat2016,
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2016},
  month = nov,
  journal = {arXiv:1604.00289 [cs, stat]},
  eprint = {1604.00289},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\faust\\Zotero\\storage\\C5E2ZH64\\Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf}
}

@article{lakeHumanlevelConceptLearning2015,
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  year = {2015},
  journal = {Science},
  volume = {350},
  number = {6266},
  pages = {1332--1338},
  doi = {DOI: 10.1126/science.aab3050},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\6VNN5Y7Y\\Lake et al. - Human-level concept learning through probabilistic.pdf}
}

@article{lakeOneShotLearning,
  title = {One Shot Learning of Simple Visual Concepts},
  author = {Lake, Brenden M and Salakhutdinov, Ruslan and Gross, Jason and Tenenbaum, Joshua B},
  pages = {7},
  abstract = {People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing stateof-the-art character model on a challenging one shot learning task, and it provides a good fit to human perceptual data.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\PMPZ575S\\Lake et al. - One shot learning of simple visual concepts.pdf}
}

@article{lakePeopleInferRecursive2020,
  title = {People {{Infer Recursive Visual Concepts}} from {{Just}} a {{Few Examples}}},
  author = {Lake, Brenden M. and Piantadosi, Steven T.},
  year = {2020},
  month = mar,
  journal = {Computational Brain \& Behavior},
  volume = {3},
  number = {1},
  pages = {54--65},
  issn = {2522-0861, 2522-087X},
  doi = {10.1007/s42113-019-00053-y},
  abstract = {Machine learning has made major advances in categorizing objects in images, yet the best algorithms miss important aspects of how people learn and think about categories. People can learn richer concepts from fewer examples, including causal models that explain how members of a category are formed. Here, we explore the limits of this human ability to infer causal ``programs''\textemdash latent generating processes with nontrivial algorithmic properties\textemdash from one, two, or three visual examples. People were asked to extrapolate the programs in several ways, for both classifying and generating new examples. As a theory of these inductive abilities, we present a Bayesian program learning model that searches the space of programs for the best explanation of the observations. Although variable, people's judgments are broadly consistent with the model and inconsistent with several alternatives, including a pretrained deep neural network for object recognition, indicating that people can learn and reason with rich algorithmic abstractions from sparse input data.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\9AQUTVHT\\Lake and Piantadosi - 2020 - People Infer Recursive Visual Concepts from Just a.pdf}
}

@article{mollicaLogicalWordLearning2021,
  title = {Logical Word Learning: {{The}} Case of Kinship},
  shorttitle = {Logical Word Learning},
  author = {Mollica, Francis and Piantadosi, Steven T.},
  year = {2021},
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-02017-5},
  abstract = {We examine the conceptual development of kinship through the lens of program induction. We present a computational model for the acquisition of kinship term concepts, resulting in the first computational model of kinship learning that is closely tied to developmental phenomena. We demonstrate that our model can learn several kinship systems of varying complexity using cross-linguistic data from English, Pukapuka, Turkish, and Yanomam\"o. More importantly, the behavioral patterns observed in children learning kinship terms, under-extension and over-generalization, fall out naturally from our learning model. We then conducted interviews to simulate realistic learning environments and demonstrate that the characteristic-to-defining shift is a consequence of our learning model in naturalistic contexts containing abstract and concrete features. We use model simulations to understand the influence of logical simplicity and children's learning environment on the order of acquisition of kinship terms, providing novel predictions for the learning trajectories of these words. We conclude with a discussion of how this model framework generalizes beyond kinship terms, as well as a discussion of its limitations.},
  langid = {english},
  pmid = {34918269},
  keywords = {Bayesian modeling,Conceptual development,Word-learning},
  file = {C\:\\Users\\faust\\Zotero\\storage\\ZMHGUK75\\Mollica and Piantadosi - 2021 - Logical word learning The case of kinship.pdf}
}

@article{overlanHierarchicalProbabilisticLanguageofThought,
  title = {A {{Hierarchical Probabilistic Language-of-Thought Model}} of {{Human Visual Concept Learning}}},
  author = {Overlan, Matthew C and Jacobs, Robert A and Piantadosi, Steven T},
  pages = {6},
  abstract = {How do people rapidly learn rich, structured concepts from sparse input? Recent approaches to concept learning have found success by integrating rules and statistics. We describe a hierarchical model in this spirit in which the rules are stochastic, generative processes, and the rules themselves arise from a higher-level stochastic, generative process. We evaluate this probabilistic language-of-thought model with data from an abstract rule learning experiment carried out with adults. In this experiment, we find novel generalization effects, and we show that the model gives a qualitatively good account of the experimental data. We then discuss the role of this kind of model in the larger context of concept learning.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\EM2S3M2T\\Overlan et al. - A Hierarchical Probabilistic Language-of-Thought M.pdf}
}

@article{overlanLearningAbstractVisual2017,
  title = {Learning Abstract Visual Concepts via Probabilistic Program Induction in a {{Language}} of {{Thought}}},
  author = {Overlan, Matthew C. and Jacobs, Robert A. and Piantadosi, Steven T.},
  year = {2017},
  month = nov,
  journal = {Cognition},
  volume = {168},
  pages = {320--334},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2017.07.005},
  abstract = {The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood. Here, we address this shortcoming by formalizing the Hierarchical Language of Thought (HLOT) model of rule learning. Given a set of data items, the model uses Bayesian inference to infer a probability distribution over stochastic programs that implement variable binding. Because the model makes use of symbolic variables as well as Bayesian inference and programs with stochastic primitives, it combines many of the advantages of both symbolic and statistical approaches to cognitive modeling. To evaluate the model, we conducted an experiment in which human subjects viewed training items and then judged which test items belong to the same concept as the training items. We found that the HLOT model provides a close match to human generalization patterns, significantly outperforming two variants of the Generalized Context Model, one variant based on string similarity and the other based on visual similarity using features from a deep convolutional neural network. Additional results suggest that variable binding happens automatically, implying that binding operations do not add complexity to peoples' hypothesized rules. Overall, this work demonstrates that a cognitive model combining symbolic variables with Bayesian inference and stochastic program primitives provides a new perspective for understanding people's patterns of generalization.},
  langid = {english},
  keywords = {Behavioral experiment,Computational modeling,Concept learning,Language of Thought,Visual learning},
  file = {C\:\\Users\\faust\\Zotero\\storage\\WCZ56YFF\\S0010027717302020.html}
}

@phdthesis{overlanProbabilisticProgramInduction,
  title = {{Probabilistic Program Induction as a Model of Human Concept Learning}},
  author = {Overlan, Matthew C.},
  address = {{Ann Arbor, United States}},
  abstract = {Humans have a unique ability to learn concepts that rely on hidden, abstract rules, that require relational reasoning, or that have latent combinatorial structure. Often these concepts systematically extend to provide infinite expressive capacity. The canonical example of such a concept is natural language, but these properties can be observed even in simple laboratory tasks. Such concept learning is interesting because it is not found in any other animal species, and it has also proven an elusive target for computational models. This thesis proposes that people represent concepts as probabilistic programs. Thus, learning such concepts corresponds to inducing a latent probabilistic program that generates, and thus explains the observed data. We explore two computational models of probabilistic program induction. The first is based on the notion of a Language of Thought, a grammar-based approach to rule-based concept learning. We adapt this framework to learn probabilistic programs and compare its predictions to the results of a human concept learning behavioral experiment. The second model is based on a novel representation of probabilistic programs as graphs. This representation brings together concepts from functional programming with the theory of probabilistic graphical models, or Bayesian networks. We test the model on two relational concept learning tasks.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9781392150252},
  langid = {Inglese},
  keywords = {Applied sciences,Concept learning,Latent combinatorial structure,Psychology,Relational reasoning},
  file = {C\:\\Users\\faust\\Zotero\\storage\\FMKUFTGL\\Overlan - Probabilistic Program Induction as a Model of Huma.pdf}
}

@book{parteeMathematicalMethodsLinguistics1990,
  title = {Mathematical Methods in Linguistics},
  author = {Partee, Barbara Hall and ter Meulen, Alice G. B. and Wall, Robert Eugene},
  year = {1990},
  series = {Studies in Linguistics and Philosophy},
  number = {v. 30},
  publisher = {{Kluwer Academic}},
  address = {{Dordrecht ; Boston}},
  isbn = {978-90-277-2244-7 978-90-277-2245-4},
  langid = {english},
  lccn = {P138 .P37 1990},
  keywords = {Mathematical linguistics},
  file = {C\:\\Users\\faust\\Zotero\\storage\\SQJ23UVL\\Partee et al. - 1990 - Mathematical methods in linguistics.pdf}
}

@article{perforsTutorialIntroductionBayesian2011,
  title = {A Tutorial Introduction to {{Bayesian}} Models of Cognitive Development},
  author = {Perfors, Amy and Tenenbaum, Joshua B. and Griffiths, Thomas L. and Xu, Fei},
  year = {2011},
  month = sep,
  journal = {Cognition},
  volume = {120},
  number = {3},
  pages = {302--321},
  issn = {00100277},
  doi = {10.1016/j.cognition.2010.11.015},
  abstract = {We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\57Y8XKGD\\Perfors et al. - 2011 - A tutorial introduction to Bayesian models of cogn.pdf}
}

@article{piantadosiBootstrappingLanguageThought2012,
  title = {Bootstrapping in a Language of Thought: {{A}} Formal Model of Numerical Concept Learning},
  author = {Piantadosi, Steven T. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  year = {2012},
  journal = {Cognition},
  volume = {123},
  number = {2},
  pages = {199--217},
  doi = {10.1016/j.cognition.2011.11.005},
  file = {C\:\\Users\\faust\\Zotero\\storage\\IQD4KBV5\\A et al. - 1997 - Temporal cognition.pdf;C\:\\Users\\faust\\Zotero\\storage\\2JDJ7JSN\\download.html}
}

@article{piantadosiComputationalOriginRepresentation2021,
  title = {The Computational Origin of Representation},
  author = {Piantadosi, Steven T.},
  year = {2021},
  month = mar,
  journal = {Minds and Machines},
  volume = {31},
  pages = {1--58},
  issn = {0924-6495},
  doi = {10.1007/s11023-020-09540-9},
  abstract = {Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations-whatever they are-must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantification, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an "assembly language" for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems.},
  langid = {english},
  pmcid = {PMC8300595},
  pmid = {34305318}
}

@article{piantadosiFourProblemsSolved2016,
  title = {Four {{Problems Solved}} by the {{Probabilistic Language}} of {{Thought}}},
  author = {Piantadosi, Steven T. and Jacobs, Robert A.},
  year = {2016},
  month = feb,
  journal = {Current Directions in Psychological Science},
  volume = {25},
  number = {1},
  pages = {54--59},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1177/0963721415609581},
  abstract = {We argue for the advantages of the probabilistic language of thought (pLOT), a recently emerging approach to modeling human cognition. Work using this framework demonstrates how the pLOT (a) refines the debate between symbols and statistics in cognitive modeling, (b) permits theories that draw on insights from both nativist and empiricist approaches, (c) explains the origins of novel and complex computational concepts, and (d) provides a framework for abstraction that can link sensation and conception. In each of these areas, the pLOT provides a productive middle ground between historical divides in cognitive psychology, pointing to a promising way forward for the field.},
  file = {C\:\\Users\\faust\\Zotero\\storage\\XEL4Y6BF\\Piantadosi and Jacobs - 2016 - Four Problems Solved by the Probabilistic Language.pdf}
}

@article{piantadosiLogicalPrimitivesThought,
  title = {The {{Logical Primitives}} of {{Thought}}: {{Empirical Foundations}} for {{Compositional Cognitive Models}}},
  author = {Piantadosi, Steven T and Tenenbaum, Joshua B and Goodman, Noah D},
  pages = {34},
  abstract = {The notion of a compositional language of thought (LOT) has been central in computational accounts of cognition from earliest attempts (Boole, 1854; Fodor, 1975) to the present day (Feldman, 2000; Penn, Holyoak, \& Povinelli, 2008; Fodor, 2008; Kemp, 2012; Goodman, Tenenbaum, \& Gerstenberg, 2015). Recent modeling work shows how statistical inferences over compositionally structured hypothesis spaces might explain learning and development across a variety of domains. However, the primitive components of such representations are typically assumed a priori by modelers and theoreticians rather than determined empirically. We show how different sets of LOT primitives, embedded in a psychologically realistic approximate Bayesian inference framework, systematically predict distinct learning curves in rule-based concept learning experiments. We use this feature of LOT models to design a set of large-scale concept learning experiments that can determine the most likely primitives for psychological concepts involving Boolean connectives and quantification. Subjects' inferences are most consistent with a rich (nonminimal) set of Boolean operations, including first-order, but not second-order, quantification. Our results more generally show how specific LOT theories can be distinguished empirically.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\T66KU2PW\\Piantadosi et al. - The Logical Primitives of Thought Empirical Found.pdf}
}

@article{piantadosiLogicalPrimitivesThought2016,
  title = {The Logical Primitives of Thought: {{Empirical}} Foundations for Compositional Cognitive Models.},
  shorttitle = {The Logical Primitives of Thought},
  author = {Piantadosi, Steven T. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  year = {2016},
  month = jul,
  journal = {Psychological Review},
  volume = {123},
  number = {4},
  pages = {392--424},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0039980},
  abstract = {The notion of a compositional language of thought (LOT) has been central in computational accounts of cognition from earliest attempts (Boole, 1854; Fodor, 1975) to the present day (Feldman, 2000; Penn, Holyoak, \& Povinelli, 2008; Fodor, 2008; Kemp, 2012; Goodman, Tenenbaum, \& Gerstenberg, 2015). Recent modeling work shows how statistical inferences over compositionally structured hypothesis spaces might explain learning and development across a variety of domains. However, the primitive components of such representations are typically assumed a priori by modelers and theoreticians rather than determined empirically. We show how different sets of LOT primitives, embedded in a psychologically realistic approximate Bayesian inference framework, systematically predict distinct learning curves in rule-based concept learning experiments. We use this feature of LOT models to design a set of large-scale concept learning experiments that can determine the most likely primitives for psychological concepts involving Boolean connectives and quantification. Subjects' inferences are most consistent with a rich (nonminimal) set of Boolean operations, including first-order, but not second-order, quantification. Our results more generally show how specific LOT theories can be distinguished empirically.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\8XEDZDIA\\Piantadosi et al. - 2016 - The logical primitives of thought Empirical found.pdf}
}

@techreport{plantonMentalCompressionBinary2020,
  type = {Preprint},
  title = {Mental Compression of Binary Sequences in a Language of Thought},
  author = {Planton, Samuel and {van Kerkoerle}, Timo and Abbih, Le{\"i}la and Maheu, Maxime and Meyniel, Florent and Sigman, Mariano and Wang, Liping and Figueira, Santiago and Romano, Sergio and Dehaene, Stanislas},
  year = {2020},
  month = jan,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/aez4w},
  abstract = {The capacity to store information in working memory strongly depends upon the ability to recode the information in a compressed form. Here, we tested the theory that human adults encode binary sequences of stimuli in memory using a recursive compression algorithm akin to a ``language of thought'', and capable of capturing nested patterns of repetitions and alternations. In five experiments, we probed memory for auditory or visual sequences using both subjective and objective measures. We used a sequence violation paradigm in which participants detected occasional violations in an otherwise fixed sequence. Both subjective ratings of complexity and objective sequence violation detection rates were well predicted by complexity, as measured by minimal description length (also known as Kolmogorov complexity) in the binary version of the ``language of geometry'', a formal language previously found to account for the human encoding of complex spatial sequences in the proposed language. We contrasted the language model with a model based solely on surprise given the stimulus transition probabilities. While both models accounted for variance in the data, the language model dominated over the transition probability model for long sequences (with a number of elements far exceeding the limits of working memory). We use model comparison to show that the minimal description length in a recursive language provides a better fit than a variety of previous encoding models for sequences. The data support the hypothesis that, beyond the extraction of statistical knowledge, human sequence coding relies on an internal compression using language-like nested structures.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\MHLT8UQT\\Planton et al. - 2020 - Mental compression of binary sequences in a langua.pdf}
}

@incollection{rescorlaLanguageThoughtHypothesis2019,
  title = {The {{Language}} of {{Thought Hypothesis}}},
  booktitle = {The {{Stanford Encyclopedia}} of {{Philosophy}}},
  author = {Rescorla, Michael},
  editor = {Zalta, Edward N.},
  year = {2019},
  edition = {Summer 2019},
  publisher = {{Metaphysics Research Lab, Stanford University}},
  abstract = {The language of thought hypothesis (LOTH) proposes thatthinking occurs in a mental language. Often called Mentalese,the mental language resembles spoken language in several key respects:it contains words that can combine into sentences; the words andsentences are meaningful; and each sentence's meaning depends ina systematic way upon the meanings of its component words and the waythose words are combined. For example, there is a Mentaleseword whale that denotes whales, and there is aMentalese word mammal that denotesmammals. These words can combine into a Mentalesesentence whales are mammals, which means thatwhales are mammals. To believe that whales are mammals is to bear anappropriate psychological relation to this sentence. During aprototypical deductive inference, I might transform the Mentalesesentence whales are mammals and the Mentalesesentence Moby Dick is a whale into theMentalese sentence Moby Dick is a mammal. As Iexecute the inference, I enter into a succession of mental states thatinstantiate those sentences., LOTH emerged gradually through the writings of Augustine, Boethius,Thomas Aquinas, John Duns Scotus, and many others. William of Ockhamoffered the first systematic treatment in his Summa Logicae(c. 1323), whichmeticulously analyzed the meaning and structure of Mentaleseexpressions. LOTH was quite popular during the late medieval era, butit slipped from view in the sixteenth and seventeenth centuries. Fromthat point through the mid-twentieth century, it played little seriousrole within theorizing about the mind., In the 1970s, LOTH underwent a dramatic revival. The watershed waspublication of Jerry Fodor's The Language of Thought(1975). Fodor argued abductively: our current best scientific theoriesof psychological activity postulate Mentalese; we therefore have goodreason to accept that Mentalese exists. Fodor's analysis exertedtremendous impact. LOTH once again became a focus of discussion, somesupportive and some critical. Debates over the existence and nature ofMentalese continue to figure prominently within philosophy andcognitive science. These debates have pivotal importance for ourunderstanding of how the mind works.},
  file = {C\:\\Users\\faust\\Zotero\\storage\\QMHRFRPK\\language-thought.html}
}

@article{romanoBayesianValidationGrammar2018,
  title = {Bayesian Validation of Grammar Productions for the Language of Thought},
  author = {Romano, Sergio and Salles, Alejo and Amalric, Marie and Dehaene, Stanislas and Sigman, Mariano and Figueira, Santiago},
  editor = {Abdo, Zaid},
  year = {2018},
  month = jul,
  journal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200420},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200420},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\BR6WSLXD\\Romano et al. - 2018 - Bayesian validation of grammar productions for the.pdf}
}

@article{ruleChildHacker2020,
  title = {The {{Child}} as {{Hacker}}},
  author = {Rule, Joshua S. and Tenenbaum, Joshua B. and Piantadosi, Steven T.},
  year = {2020},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {24},
  number = {11},
  pages = {900--915},
  issn = {1879-307X},
  doi = {10.1016/j.tics.2020.07.005},
  abstract = {The scope of human learning and development poses a radical challenge for cognitive science. We propose that developmental theories can address this challenge by adopting perspectives from computer science. Many of our best models treat learning as analogous to computer programming because symbolic programs provide the most compelling account of sophisticated mental representations. We specifically propose that children's learning is analogous to a particular style of programming called hacking, making code better along many dimensions through an open-ended set of goals and activities. By contrast to existing theories, which depend primarily on local search and simple metrics, this view highlights the many features of good mental representations and the multiple complementary processes children use to create them.},
  langid = {english},
  pmcid = {PMC7673661},
  pmid = {33012688},
  keywords = {Child,Child Development,Cognitive Science,Computational modeling,Hacking,Humans,Language of thought,Learning,Learning and cognitive development,Models; Psychological,Program induction},
  file = {C\:\\Users\\faust\\Zotero\\storage\\H3UG4UFM\\Rule et al. - 2020 - The Child as Hacker.pdf}
}

@article{schneiderLanguageThought,
  title = {The {{Language}} of {{Thought}}},
  author = {Schneider, Susan},
  pages = {16},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\QT9XHGZR\\Schneider - The L ang u a g e o f Thou gh t.pdf}
}

@article{tenenbaumGeneralizationSimilarityBayesian2001,
  title = {Generalization, Similarity, and {{Bayesian}} Inference},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = {2001},
  month = aug,
  journal = {Behavioral and Brain Sciences},
  volume = {24},
  number = {4},
  pages = {629--640},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X01000061},
  abstract = {Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. Here we recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization. This unification allows us not only to draw deep parallels between the set-theoretic and spatial approaches, but also to significantly advance the explanatory power of set-theoretic models.},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\KU8LVP3K\\Tenenbaum and Griffiths - 2001 - Generalization, similarity, and Bayesian inference.pdf}
}

@article{tenenbaumHowGrowMind2011,
  title = {How to {{Grow}} a {{Mind}}: {{Statistics}}, {{Structure}}, and {{Abstraction}}},
  shorttitle = {How to {{Grow}} a {{Mind}}},
  author = {Tenenbaum, Joshua B. and Kemp, Charles and Griffiths, Thomas L. and Goodman, Noah D.},
  year = {2011},
  month = mar,
  journal = {Science},
  volume = {331},
  number = {6022},
  pages = {1279--1285},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.1192788},
  file = {C\:\\Users\\faust\\Zotero\\storage\\ZAUPZ9W2\\Tenenbaum et al. - 2011 - How to Grow a Mind Statistics, Structure, and Abs.pdf}
}

@article{ullmanBayesianModelsConceptual2020,
  title = {Bayesian {{Models}} of {{Conceptual Development}}: {{Learning}} as {{Building Models}} of the {{World}}},
  shorttitle = {Bayesian {{Models}} of {{Conceptual Development}}},
  author = {Ullman, Tomer D. and Tenenbaum, Joshua B.},
  year = {2020},
  journal = {Annual Review of Developmental Psychology},
  volume = {2},
  number = {1},
  pages = {533--558},
  doi = {10.1146/annurev-devpsych-121318-084833},
  abstract = {A Bayesian framework helps address, in computational terms, what knowledge children start with and how they construct and adapt models of the world during childhood. Within this framework, inference over hierarchies of probabilistic generative programs in particular offers a normative and descriptive account of children's model building. We consider two classic settings in which cognitive development has been framed as model building: (a) core knowledge in infancy and (b) the child as scientist. We interpret learning in both of these settings as resource-constrained, hierarchical Bayesian program induction with different primitives and constraints. We examine what mechanisms children could use to meet the algorithmic challenges of navigating large spaces of potential models, in particular the proposal of the child as hacker and how it might be realized by drawing on recent computational advances. We also discuss prospects for a unifying account of model building across scientific theories and intuitive theories, and in biological and cultural evolution more generally.},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-devpsych-121318-084833},
  file = {C\:\\Users\\faust\\Zotero\\storage\\L448NX7J\\Ullman and Tenenbaum - 2020 - Bayesian Models of Conceptual Development Learnin.pdf}
}

@article{xuWordLearningBayesian2007,
  title = {Word Learning as {{Bayesian}} Inference.},
  author = {Xu, Fei and Tenenbaum, Joshua B.},
  year = {2007},
  journal = {Psychological Review},
  volume = {114},
  number = {2},
  pages = {245--272},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.114.2.245},
  langid = {english},
  file = {C\:\\Users\\faust\\Zotero\\storage\\453CI2R8\\Xu and Tenenbaum - 2007 - Word learning as Bayesian inference..pdf}
}

@article{zhouFlexibleCompositionalLearning2021,
  title = {Flexible {{Compositional Learning}} of {{Structured Visual Concepts}}},
  author = {Zhou, Yanli and Lake, Brenden M.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.09848 [cs]},
  eprint = {2105.09848},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Humans are highly efficient learners, with the ability to grasp the meaning of a new concept from just a few examples. Unlike popular computer vision systems, humans can flexibly leverage the compositional structure of the visual world, understanding new concepts as combinations of existing concepts. In the current paper, we study how people learn different types of visual compositions, using abstract visual forms with rich relational structure. We find that people can make meaningful compositional generalizations from just a few examples in a variety of scenarios, and we develop a Bayesian program induction model that provides a close fit to the behavioral data. Unlike past work examining special cases of compositionality, our work shows how a single computational approach can account for many distinct types of compositional generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\faust\\Zotero\\storage\\7KKEYTXE\\Zhou and Lake - 2021 - Flexible Compositional Learning of Structured Visu.pdf;C\:\\Users\\faust\\Zotero\\storage\\8SSGYRM9\\2105.html}
}


